{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import layers\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_games_list(filename='corpus.txt', verbose = False):\n",
    "    '''\n",
    "    in: filename, the corpus\n",
    "    out: list of list of events in each game\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('Reading corpus')\n",
    "    with open(filename,'r') as f:\n",
    "        return [game.split() for game in f.readlines()]\n",
    "    \n",
    "def strip_aZ(games_list):\n",
    "    '''\n",
    "    in: list of lists of games\n",
    "    out: stripped to just ^a-zA-Z\n",
    "    '''\n",
    "    print('Stripping games_list of non-[^a-zA-Z] characters')\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    games_list = [[regex.sub('',event) for event in game] for game in games_list]\n",
    "    #print(games_list[:3][:5])\n",
    "    return games_list\n",
    "    \n",
    "    \n",
    "def make_vocabulary(games_list, verbose=False):\n",
    "    '''\n",
    "    in: list of lists of games\n",
    "    out: \n",
    "        vocabulary, the number of distinct events\n",
    "            and\n",
    "        event_2_ind, a lookup dictionary for the index of each event\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('Constructing vocabular and event2id dictionary')\n",
    "    \n",
    "    #full events list to make vocabulary and ids\n",
    "    events = flatten_games_to_events(games_list)\n",
    "    #select distinct\n",
    "    distinct_events = list(set(events))\n",
    "    vocabulary = len(distinct_events)\n",
    "    #make id dictionary\n",
    "    event_2_id = {}\n",
    "    for event in distinct_events:\n",
    "        event_2_id[event] = distinct_events.index(event)\n",
    "    id_2_event = dict(zip(event_2_id.values(), event_2_id.keys()))\n",
    "    return vocabulary, event_2_id, id_2_event\n",
    "   \n",
    "def games_list_to_ids(games_list, event_2_id, verbose=False):\n",
    "    '''\n",
    "    in: games_list, list of lists of events in string format\n",
    "        event_2_id, id dictionary constructed from full vocabulary\n",
    "    out: \n",
    "        games_list, list of lists of events in id format\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('Encoding a list of games by event id')\n",
    "    return [[event_2_id[event] for event in game] for game in games_list]\n",
    "\n",
    "def id_list_to_event(id_list, id_2_event, verbose=False):\n",
    "    '''\n",
    "    in: id_list a list of events in id format\n",
    "        id_2_event: lookup dictionary\n",
    "    out: \n",
    "        event_list: a list of events in event format\n",
    "    '''\n",
    "    return [id_2_event[idd] for idd in id_list ]\n",
    "\n",
    "def train_test_split_games_list(games_list, train_frac = .8, verbose=False):\n",
    "    if verbose:\n",
    "        print(f'Making train test split. train_frac = {train_frac}')\n",
    "    split_ind = int(len(games_list)*train_frac)\n",
    "    train = games_list[:split_ind]\n",
    "    test = games_list[split_ind:]\n",
    "    return train, test\n",
    "\n",
    "def flatten_games_to_events(games_list): \n",
    "    return [event for game in games_list\n",
    "                     for event in game]\n",
    "\n",
    "def load_data(filename):\n",
    "    '''\n",
    "    in: filename, a .txt file whose lines are nhl games where events\n",
    "    are represented by strings separated by spaces\n",
    "    '''\n",
    "    games_list = load_games_list('corpus.txt', \n",
    "                                 verbose=True)[:200]\n",
    "    \n",
    "    ###This is to make things fast for now\n",
    "    games_list = strip_aZ(games_list)\n",
    "    \n",
    "    #building word to index dictionary and vocabulary\n",
    "    vocabulary, event_2_id, id_2_event = make_vocabulary(games_list, \n",
    "                                                         verbose=True)\n",
    "    \n",
    "    #convert to ids\n",
    "    games_list = games_list_to_ids(games_list, \n",
    "                                   event_2_id, \n",
    "                                   verbose=True)\n",
    "    \n",
    "    #train test split\n",
    "    train_data, test_data = train_test_split_games_list(games_list, \n",
    "                                                        verbose=True)\n",
    "    #flatten training (testing) data to list of events\n",
    "    train_data = flatten_games_to_events(train_data)\n",
    "    test_data = flatten_games_to_events(test_data)\n",
    "    valid_data = None\n",
    "    \n",
    "    reversed_dictionary = None\n",
    "    \n",
    "    return (\n",
    "            train_data, \n",
    "            valid_data, \n",
    "            test_data, \n",
    "            vocabulary, \n",
    "            reversed_dictionary, \n",
    "            event_2_id,\n",
    "            id_2_event\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "    '''\n",
    "    generates batches for Keras to train neural networks \n",
    "    should I grab the batches randomly?\n",
    "    '''\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.current_idx = 0\n",
    "        self.skip_step = skip_step\n",
    "        \n",
    "    def generate(self):\n",
    "        #input is just the number of steps in each in, and the batch size\n",
    "        x = np.zeros((self.batch_size, \n",
    "                      self.num_steps))\n",
    "        #output will be one-hots of dimension vocabulary\n",
    "        y = np.zeros((self.batch_size, \n",
    "                      self.num_steps, \n",
    "                      self.vocabulary))\n",
    "        while True:#never terminate\n",
    "            for i in range(self.batch_size):\n",
    "                #if I would run over the edge, reset idx\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    self.current_idx = 0\n",
    "                x[i,:] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps+1]\n",
    "                #make the one-hots for the y training data\n",
    "                y[i,:,:] = to_categorical(temp_y, \n",
    "                                          num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_LSTM_RNN(vocabulary, hidden_size, num_steps, use_dropout=True):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocabulary,\n",
    "                               hidden_size,\n",
    "                               input_length=num_steps))\n",
    "    model.add(layers.LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(layers.LSTM(hidden_size, return_sequences=True))\n",
    "    if use_dropout:\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(vocabulary,\n",
    "                                                  activation='softmax')))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus\n",
      "Stripping games_list of non-[^a-zA-Z] characters\n",
      "Constructing vocabular and event2id dictionary\n",
      "Encoding a list of games by event id\n",
      "Making train test split. train_frac = 0.8\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary, event_2_id, id_2_event = load_data('corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_it(name, obj):\n",
    "    with open('{}.pkl'.format(name), 'wb') as f:\n",
    "        pickle.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('event_2_id.pkl', 'wb') as f:\n",
    "   pickle.dump(event_2_id, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PeriodOfficialnocoords': 0,\n",
       " 'Shotnocoords': 1,\n",
       " 'PeriodEndnocoords': 2,\n",
       " 'Giveawaynocoords': 3,\n",
       " 'Hitnocoords': 4,\n",
       " 'Penalty': 5,\n",
       " 'ShootoutCompletenocoords': 6,\n",
       " 'Goal': 7,\n",
       " 'Faceoff': 8,\n",
       " 'Penaltynocoords': 9,\n",
       " 'GameOfficialnocoords': 10,\n",
       " 'MissedShot': 11,\n",
       " 'BlockedShotnocoords': 12,\n",
       " 'Hit': 13,\n",
       " 'BlockedShot': 14,\n",
       " 'Takeawaynocoords': 15,\n",
       " 'Takeaway': 16,\n",
       " 'MissedShotnocoords': 17,\n",
       " 'Stoppagenocoords': 18,\n",
       " 'Goalnocoords': 19,\n",
       " 'Shot': 20,\n",
       " 'GameEndnocoords': 21,\n",
       " 'GameSchedulednocoords': 22,\n",
       " 'Faceoffnocoords': 23,\n",
       " 'PeriodStartnocoords': 24,\n",
       " 'Giveaway': 25,\n",
       " 'PeriodReadynocoords': 26}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_2_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 150\n",
    "batch_size = 10\n",
    "train_data_generator = KerasBatchGenerator(train_data, \n",
    "                                           num_steps, \n",
    "                                           batch_size,\n",
    "                                           vocabulary,\n",
    "                                           skip_step = num_steps)\n",
    "\n",
    "test_data_generator = KerasBatchGenerator(test_data, \n",
    "                                          num_steps, \n",
    "                                          batch_size,\n",
    "                                          vocabulary,\n",
    "                                          skip_step = num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 20\n",
    "model = make_LSTM_RNN(vocabulary, hidden_size, num_steps)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should write a definition, get callbacks, that returns a list of callbacks\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='model-{epoch:02d}.hdf5',\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7331 - categorical_accuracy: 0.4033\n",
      "Epoch 00001: saving model to model-01.hdf5\n",
      "33/33 [==============================] - 10s 309ms/step - loss: 1.7340 - categorical_accuracy: 0.4031 - val_loss: 1.6769 - val_categorical_accuracy: 0.4126\n",
      "Epoch 2/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7319 - categorical_accuracy: 0.4039\n",
      "Epoch 00002: saving model to model-02.hdf5\n",
      "33/33 [==============================] - 10s 308ms/step - loss: 1.7317 - categorical_accuracy: 0.4042 - val_loss: 1.6811 - val_categorical_accuracy: 0.4121\n",
      "Epoch 3/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7368 - categorical_accuracy: 0.4032\n",
      "Epoch 00003: saving model to model-03.hdf5\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 1.7367 - categorical_accuracy: 0.4037 - val_loss: 1.6795 - val_categorical_accuracy: 0.4124\n",
      "Epoch 4/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7300 - categorical_accuracy: 0.4038\n",
      "Epoch 00004: saving model to model-04.hdf5\n",
      "33/33 [==============================] - 10s 309ms/step - loss: 1.7304 - categorical_accuracy: 0.4038 - val_loss: 1.6754 - val_categorical_accuracy: 0.4151\n",
      "Epoch 5/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7330 - categorical_accuracy: 0.4036\n",
      "Epoch 00005: saving model to model-05.hdf5\n",
      "33/33 [==============================] - 10s 302ms/step - loss: 1.7336 - categorical_accuracy: 0.4036 - val_loss: 1.6801 - val_categorical_accuracy: 0.4112\n",
      "Epoch 6/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7312 - categorical_accuracy: 0.4031\n",
      "Epoch 00006: saving model to model-06.hdf5\n",
      "33/33 [==============================] - 10s 307ms/step - loss: 1.7301 - categorical_accuracy: 0.4036 - val_loss: 1.6800 - val_categorical_accuracy: 0.4124\n",
      "Epoch 7/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7320 - categorical_accuracy: 0.4038\n",
      "Epoch 00007: saving model to model-07.hdf5\n",
      "33/33 [==============================] - 10s 316ms/step - loss: 1.7315 - categorical_accuracy: 0.4038 - val_loss: 1.6762 - val_categorical_accuracy: 0.4141\n",
      "Epoch 8/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7298 - categorical_accuracy: 0.4039\n",
      "Epoch 00008: saving model to model-08.hdf5\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 1.7334 - categorical_accuracy: 0.4036 - val_loss: 1.6814 - val_categorical_accuracy: 0.4123\n",
      "Epoch 9/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7272 - categorical_accuracy: 0.4030\n",
      "Epoch 00009: saving model to model-09.hdf5\n",
      "33/33 [==============================] - 10s 314ms/step - loss: 1.7302 - categorical_accuracy: 0.4032 - val_loss: 1.6783 - val_categorical_accuracy: 0.4131\n",
      "Epoch 10/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7322 - categorical_accuracy: 0.4042\n",
      "Epoch 00010: saving model to model-10.hdf5\n",
      "33/33 [==============================] - 10s 306ms/step - loss: 1.7331 - categorical_accuracy: 0.4041 - val_loss: 1.6761 - val_categorical_accuracy: 0.4156\n",
      "Epoch 11/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7289 - categorical_accuracy: 0.4037\n",
      "Epoch 00011: saving model to model-11.hdf5\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 1.7300 - categorical_accuracy: 0.4036 - val_loss: 1.6830 - val_categorical_accuracy: 0.4111\n",
      "Epoch 12/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7349 - categorical_accuracy: 0.4039\n",
      "Epoch 00012: saving model to model-12.hdf5\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 1.7418 - categorical_accuracy: 0.4032 - val_loss: 1.6771 - val_categorical_accuracy: 0.4129\n",
      "Epoch 13/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7295 - categorical_accuracy: 0.4035\n",
      "Epoch 00013: saving model to model-13.hdf5\n",
      "33/33 [==============================] - 10s 309ms/step - loss: 1.7288 - categorical_accuracy: 0.4041 - val_loss: 1.6762 - val_categorical_accuracy: 0.4142\n",
      "Epoch 14/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7305 - categorical_accuracy: 0.4040\n",
      "Epoch 00014: saving model to model-14.hdf5\n",
      "33/33 [==============================] - 10s 303ms/step - loss: 1.7310 - categorical_accuracy: 0.4042 - val_loss: 1.6855 - val_categorical_accuracy: 0.4123\n",
      "Epoch 15/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7289 - categorical_accuracy: 0.4044\n",
      "Epoch 00015: saving model to model-15.hdf5\n",
      "33/33 [==============================] - 10s 312ms/step - loss: 1.7287 - categorical_accuracy: 0.4038 - val_loss: 1.6781 - val_categorical_accuracy: 0.4129\n",
      "Epoch 16/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7356 - categorical_accuracy: 0.4037\n",
      "Epoch 00016: saving model to model-16.hdf5\n",
      "33/33 [==============================] - 10s 310ms/step - loss: 1.7344 - categorical_accuracy: 0.4039 - val_loss: 1.6746 - val_categorical_accuracy: 0.4150\n",
      "Epoch 17/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7335 - categorical_accuracy: 0.4045\n",
      "Epoch 00017: saving model to model-17.hdf5\n",
      "33/33 [==============================] - 10s 315ms/step - loss: 1.7331 - categorical_accuracy: 0.4047 - val_loss: 1.6769 - val_categorical_accuracy: 0.4138\n",
      "Epoch 18/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7274 - categorical_accuracy: 0.4039\n",
      "Epoch 00018: saving model to model-18.hdf5\n",
      "33/33 [==============================] - 10s 310ms/step - loss: 1.7275 - categorical_accuracy: 0.4036 - val_loss: 1.6764 - val_categorical_accuracy: 0.4129\n",
      "Epoch 19/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7247 - categorical_accuracy: 0.4052\n",
      "Epoch 00019: saving model to model-19.hdf5\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 1.7254 - categorical_accuracy: 0.4050 - val_loss: 1.6765 - val_categorical_accuracy: 0.4133\n",
      "Epoch 20/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7253 - categorical_accuracy: 0.4038\n",
      "Epoch 00020: saving model to model-20.hdf5\n",
      "33/33 [==============================] - 10s 303ms/step - loss: 1.7270 - categorical_accuracy: 0.4035 - val_loss: 1.6800 - val_categorical_accuracy: 0.4118\n",
      "Epoch 21/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7220 - categorical_accuracy: 0.4055\n",
      "Epoch 00021: saving model to model-21.hdf5\n",
      "33/33 [==============================] - 10s 310ms/step - loss: 1.7226 - categorical_accuracy: 0.4056 - val_loss: 1.6775 - val_categorical_accuracy: 0.4119\n",
      "Epoch 22/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7317 - categorical_accuracy: 0.4039\n",
      "Epoch 00022: saving model to model-22.hdf5\n",
      "33/33 [==============================] - 11s 321ms/step - loss: 1.7307 - categorical_accuracy: 0.4044 - val_loss: 1.6974 - val_categorical_accuracy: 0.4123\n",
      "Epoch 23/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7343 - categorical_accuracy: 0.4025\n",
      "Epoch 00023: saving model to model-23.hdf5\n",
      "33/33 [==============================] - 10s 308ms/step - loss: 1.7332 - categorical_accuracy: 0.4032 - val_loss: 1.6807 - val_categorical_accuracy: 0.4127\n",
      "Epoch 24/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7243 - categorical_accuracy: 0.4054\n",
      "Epoch 00024: saving model to model-24.hdf5\n",
      "33/33 [==============================] - 10s 308ms/step - loss: 1.7245 - categorical_accuracy: 0.4054 - val_loss: 1.6776 - val_categorical_accuracy: 0.4127\n",
      "Epoch 25/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7222 - categorical_accuracy: 0.4046\n",
      "Epoch 00025: saving model to model-25.hdf5\n",
      "33/33 [==============================] - 11s 322ms/step - loss: 1.7219 - categorical_accuracy: 0.4042 - val_loss: 1.6771 - val_categorical_accuracy: 0.4131\n",
      "Epoch 26/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7274 - categorical_accuracy: 0.4028\n",
      "Epoch 00026: saving model to model-26.hdf5\n",
      "33/33 [==============================] - 10s 297ms/step - loss: 1.7334 - categorical_accuracy: 0.4026 - val_loss: 1.6770 - val_categorical_accuracy: 0.4135\n",
      "Epoch 27/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7250 - categorical_accuracy: 0.4019\n",
      "Epoch 00027: saving model to model-27.hdf5\n",
      "33/33 [==============================] - 10s 308ms/step - loss: 1.7252 - categorical_accuracy: 0.4027 - val_loss: 1.6757 - val_categorical_accuracy: 0.4123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7238 - categorical_accuracy: 0.4028\n",
      "Epoch 00028: saving model to model-28.hdf5\n",
      "33/33 [==============================] - 10s 306ms/step - loss: 1.7249 - categorical_accuracy: 0.4022 - val_loss: 1.6765 - val_categorical_accuracy: 0.4140\n",
      "Epoch 29/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7219 - categorical_accuracy: 0.4058\n",
      "Epoch 00029: saving model to model-29.hdf5\n",
      "33/33 [==============================] - 10s 313ms/step - loss: 1.7234 - categorical_accuracy: 0.4046 - val_loss: 1.6754 - val_categorical_accuracy: 0.4152\n",
      "Epoch 30/30\n",
      "32/33 [============================>.] - ETA: 0s - loss: 1.7231 - categorical_accuracy: 0.4051\n",
      "Epoch 00030: saving model to model-30.hdf5\n",
      "33/33 [==============================] - 10s 309ms/step - loss: 1.7230 - categorical_accuracy: 0.4053 - val_loss: 1.6774 - val_categorical_accuracy: 0.4132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4de25c0a90>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "model.fit_generator(train_data_generator.generate(), \n",
    "                    len(train_data)//(batch_size*num_steps), \n",
    "                    num_epochs, \n",
    "                    validation_data=test_data_generator.generate(),\n",
    "                    validation_steps=len(test_data)//(batch_size*num_steps),\n",
    "                    callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_model(trained_LSTM_RNN, vocabulary, hidden_size):\n",
    "    model_predicting = make_LSTM_RNN(vocabulary, hidden_size, None)\n",
    "    model_predicting.set_weights(trained_LSTM_RNN.get_weights())\n",
    "    return model_predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictining = make_prediction_model(model, vocabulary, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_probs(seed_list, model_predictining):\n",
    "    '''\n",
    "    seed_list is the game so far in event format\n",
    "    '''\n",
    "    model_predictining.reset_states()\n",
    "    for seed in seed_list[:-1]:\n",
    "        model_predictining.predict([seed,], verbose=0)\n",
    "    probs_vector = model_predictining.predict([seed_list[-1],],\n",
    "                                          verbose=1)[0][0]\n",
    "    probs = {}\n",
    "    for i, prob in enumerate(probs_vector):\n",
    "        probs[id_2_event[i]]=prob\n",
    "    return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seed_list_id = [test_data[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GameSchedulednocoords']\n"
     ]
    }
   ],
   "source": [
    "test_seed_list = id_list_to_event(test_seed_list_id,id_2_event)\n",
    "print(test_seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "nps = next_probs(test_data[:10], model_predictining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BlockedShot': 0.0062115295,\n",
      " 'BlockedShotnocoords': 0.00041467499,\n",
      " 'Faceoff': 0.856352,\n",
      " 'Faceoffnocoords': 0.03678408,\n",
      " 'GameEndnocoords': 7.033351e-05,\n",
      " 'GameOfficialnocoords': 0.0004248029,\n",
      " 'GameSchedulednocoords': 0.00010391814,\n",
      " 'Giveaway': 0.0023818028,\n",
      " 'Giveawaynocoords': 0.00043035016,\n",
      " 'Goal': 0.003245439,\n",
      " 'Goalnocoords': 0.00042935443,\n",
      " 'Hit': 0.004769465,\n",
      " 'Hitnocoords': 0.00051336444,\n",
      " 'MissedShot': 0.005453769,\n",
      " 'MissedShotnocoords': 0.00058681925,\n",
      " 'Penalty': 0.02522075,\n",
      " 'Penaltynocoords': 0.0081690885,\n",
      " 'PeriodEndnocoords': 0.0018994793,\n",
      " 'PeriodOfficialnocoords': 3.5807486e-06,\n",
      " 'PeriodReadynocoords': 9.251992e-05,\n",
      " 'PeriodStartnocoords': 6.187619e-05,\n",
      " 'ShootoutCompletenocoords': 0.0010916325,\n",
      " 'Shot': 0.018091103,\n",
      " 'Shotnocoords': 0.00089533883,\n",
      " 'Stoppagenocoords': 0.023498546,\n",
      " 'Takeaway': 0.0020847202,\n",
      " 'Takeawaynocoords': 0.0007197218}\n"
     ]
    }
   ],
   "source": [
    "pprint(nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stoppagenocoords'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_index = np.random.choice(vocabulary, p=nps[0][0])\n",
    "id_2_event[test_data[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PeriodOfficialnocoords',\n",
       " 1: 'Shotnocoords',\n",
       " 2: 'PeriodEndnocoords',\n",
       " 3: 'Giveawaynocoords',\n",
       " 4: 'Hitnocoords',\n",
       " 5: 'Penalty',\n",
       " 6: 'ShootoutCompletenocoords',\n",
       " 7: 'Goal',\n",
       " 8: 'Faceoff',\n",
       " 9: 'Penaltynocoords',\n",
       " 10: 'GameOfficialnocoords',\n",
       " 11: 'MissedShot',\n",
       " 12: 'BlockedShotnocoords',\n",
       " 13: 'Hit',\n",
       " 14: 'BlockedShot',\n",
       " 15: 'Takeawaynocoords',\n",
       " 16: 'Takeaway',\n",
       " 17: 'MissedShotnocoords',\n",
       " 18: 'Stoppagenocoords',\n",
       " 19: 'Goalnocoords',\n",
       " 20: 'Shot',\n",
       " 21: 'GameEndnocoords',\n",
       " 22: 'GameSchedulednocoords',\n",
       " 23: 'Faceoffnocoords',\n",
       " 24: 'PeriodStartnocoords',\n",
       " 25: 'Giveaway',\n",
       " 26: 'PeriodReadynocoords'}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_2_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-18905c09f66f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_predictining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/data3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/data3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/data3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         raise ImportError(\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_predictining, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.14\n",
      "  latest version: 4.7.8\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - pydot\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _libgcc_mutex-0.1          |             main           3 KB  defaults\n",
      "    cairo-1.16.0               |    h18b612c_1001         1.5 MB  conda-forge\n",
      "    certifi-2019.6.16          |           py37_1         149 KB  conda-forge\n",
      "    expat-2.2.5                |    he1b5a44_1003         191 KB  conda-forge\n",
      "    fribidi-1.0.5              |    h516909a_1002         112 KB  conda-forge\n",
      "    glib-2.58.3                |    h6f030ca_1002         3.3 MB  conda-forge\n",
      "    graphviz-2.40.1            |       h5933667_1         6.4 MB  conda-forge\n",
      "    harfbuzz-2.4.0             |       h37c48d4_1         1.5 MB  conda-forge\n",
      "    libtiff-4.0.10             |    h57b8799_1003         587 KB  conda-forge\n",
      "    libtool-2.4.6              |    h14c3975_1002         512 KB  conda-forge\n",
      "    libxml2-2.9.9              |       h13577e0_1         1.3 MB  conda-forge\n",
      "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
      "    openssl-1.1.1c             |       h516909a_0         2.1 MB  conda-forge\n",
      "    pango-1.42.4               |       he7ab937_0         515 KB  conda-forge\n",
      "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
      "    pydot-1.4.1                |        py37_1000          40 KB  conda-forge\n",
      "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
      "    xorg-libx11-1.6.8          |       h516909a_0         907 KB  conda-forge\n",
      "    xorg-libxpm-3.5.12         |    h14c3975_1002          64 KB  conda-forge\n",
      "    xorg-libxt-1.1.5           |    h516909a_1003         367 KB  conda-forge\n",
      "    zstd-1.4.0                 |       h3b9ef0a_0         928 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        21.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
      "  cairo              conda-forge/linux-64::cairo-1.16.0-h18b612c_1001\n",
      "  expat              conda-forge/linux-64::expat-2.2.5-he1b5a44_1003\n",
      "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-he4413a7_1000\n",
      "  freetype           conda-forge/linux-64::freetype-2.10.0-he983fc9_0\n",
      "  fribidi            conda-forge/linux-64::fribidi-1.0.5-h516909a_1002\n",
      "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
      "  glib               conda-forge/linux-64::glib-2.58.3-h6f030ca_1002\n",
      "  graphite2          conda-forge/linux-64::graphite2-1.3.13-hf484d3e_1000\n",
      "  graphviz           conda-forge/linux-64::graphviz-2.40.1-h5933667_1\n",
      "  harfbuzz           conda-forge/linux-64::harfbuzz-2.4.0-h37c48d4_1\n",
      "  icu                conda-forge/linux-64::icu-58.2-hf484d3e_1000\n",
      "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
      "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1005\n",
      "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_0\n",
      "  libtiff            conda-forge/linux-64::libtiff-4.0.10-h57b8799_1003\n",
      "  libtool            conda-forge/linux-64::libtool-2.4.6-h14c3975_1002\n",
      "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
      "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
      "  libxml2            conda-forge/linux-64::libxml2-2.9.9-h13577e0_1\n",
      "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
      "  pango              conda-forge/linux-64::pango-1.42.4-he7ab937_0\n",
      "  pcre               conda-forge/linux-64::pcre-8.41-hf484d3e_1003\n",
      "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
      "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
      "  pydot              conda-forge/linux-64::pydot-1.4.1-py37_1000\n",
      "  pyparsing          conda-forge/noarch::pyparsing-2.4.0-py_0\n",
      "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
      "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
      "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
      "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.8-h516909a_0\n",
      "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
      "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
      "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
      "  xorg-libxpm        conda-forge/linux-64::xorg-libxpm-3.5.12-h14c3975_1002\n",
      "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
      "  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.1.5-h516909a_1003\n",
      "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
      "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
      "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
      "  zstd               conda-forge/linux-64::zstd-1.4.0-h3b9ef0a_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi                                  2019.6.16-py37_0 --> 2019.6.16-py37_1\n",
      "  openssl                                 1.1.1b-h14c3975_1 --> 1.1.1c-h516909a_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
